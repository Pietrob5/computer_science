

<h1 align="center">Informatica</h1>
<h4 align="center">
    <p>
        <b>Italiano</b> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README.md">English</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_es.md">Español</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_fr.md">Français</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_bn.md">বাংলা</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_ta.md">தமிழ்</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_guj.md">ગુજરાતી</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_pt.md">Portuguese</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_hi.md">हिंदी</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_te.md">తెలుగు</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_ro.md">Română</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_ar.md">العربية</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_np.md">Nepali</a> |
        <a href="https://github.com/shhossain/computer_science/blob/main/README_cn.md">简体中文</a>
    </p>
</h4>

## Linee guida per i contributi
Se sei interessato a contribuire a questo progetto, prenditi un momento per leggere [CONTRIBUTING.md](https://github.com/shhossain/computer_science/blob/main/CONTRIBUTING.md) per istruzioni dettagliate su come iniziare. I tuoi contributi sono molto apprezzati!

<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->
[![All Contributors](https://img.shields.io/badge/all_contributors-153-orange.svg?style=flat-square)](#contributors-)
<!-- ALL-CONTRIBUTORS-BADGE:END -->

## Indice dei Contenuti

- [Introduzione](#introduzione)
- [Computer Elettronico](#computer-elettronico)
- [Logica Booleana](#logica-booleana)
- [Circuiti Digitali](#circuiti-digitali)
- [Sistemi Numerici](#sistemi-numerici)
- [Unità di Elaborazione Centrale (CPU)](#unità-di-elaborazione-centrale-cpu)
- [Registri, Cache e RAM](#registri-cache-e-ram)
- [Istruzioni e Programma](#istruzioni-e-programma)
- [Linguaggi di Programmazione](#linguaggi-di-programmazione)
- [Tipi di Dati](#tipi-di-dati)
- [Istruzioni e Funzioni](#istruzioni-e-funzioni)
- [Strutture Dati](#strutture-dati)
- [Algoritmi](#algoritmi)
- [Alan Turing](#alan-turing)
- [Ingegneria del Software](#ingegneria-del-software)
- [Scienza dei Dati](#scienza-dei-dati)
- [Circuiti Integrati](#circuiti-integrati)
- [Programmazione Orientata agli Oggetti](#programmazione-orientata-agli-oggetti)
- [Programmazione Funzionale](#programmazione-funzionale)
- [Sistemi Operativi](#sistemi-operativi)
- [Memoria e Archiviazione](#memoria-e-archiviazione)
- [File System](#file-system)
- [Cloud Computing](#cloud-computing)
- [Apprendimento Automatico](#apprendimento-automatico)
- [Tecnologia Web](#tecnologia-web)
- [Reti](#reti)
- [Internet](#internet)
- [DBMS (Sistema di Gestione di Database)](#dbms)
- [Crittografia](#crittografia)
- [Teoria della Computazione](#teoria-della-computazione)


## Introduzione

L'informatica è lo studio dei computer e del calcolo e delle loro applicazioni teoriche e pratiche. L'informatica applica i principi della matematica, dell'ingegneria e della logica a una moltitudine di problemi, tra cui la formulazione di algoritmi, lo sviluppo di software/hardware e l'intelligenza artificiale.


## [Computer Elettronico](Electronic%20Computer/readme.md)
Un dispositivo che esegue calcoli, in particolare una macchina elettronica programmabile che esegue operazioni matematiche o logiche ad alta velocità, o che assembla, memorizza, correla o elabora in altro modo informazioni.

## [Logica Booleana](Boolean%20Logic/readme.md)
La logica booleana è un ramo della matematica che si occupa dei valori di verità e falsità. È un sistema di logica che utilizza solo due valori, 0 e 1, per rappresentare rispettivamente falso e vero. È anche conosciuta come algebra booleana, dal nome di George Boole, che l'ha descritta per la prima volta nel 1854.

### Operatori Booleani Comuni
| Operatore | Nome |               Descrizione               |
| :------: | :--: | :-------------------------------------: |
|    !     | NOT  |    Nega il valore dell'operando.    |
|    &&    | AND  | Restituisce **vero** se entrambi gli operandi sono veri. |
|   \|\|   |  OR  | Restituisce **vero** se uno o entrambi gli operandi sono veri. |

### Operatori Booleani Interessanti
| Operatore | Nome |               Descrizione               |
| :------: | :--: | :-------------------------------------: |
|    ()    | Parentesi     |   Permette di raggruppare parole chiave e controllare l'ordine in cui i termini verranno cercati.    |
|    “”    | Virgolette | Fornisce risultati con la frase esatta. |
|   *      |  Asterisco       | Fornisce risultati contenenti una variazione della parola chiave. |
|   ⊕     |  XOR            | Restituisce **vero** se gli operandi sono diversi |
|   ⊽      |  NOR            | Restituisce **vero** se tutti gli operandi sono falsi. |
|   ⊼      |  NAND           | Restituisce **falso** solo se entrambi i valori dei suoi due input sono veri. |

## [Circuiti Digitali](Digital%20Circuits/readme.md)

I circuiti digitali si occupano di segnali booleani (1 e 0). Sono i blocchi fondamentali di un computer. Sono i componenti e i circuiti utilizzati per creare unità di processore e unità di memoria essenziali per un sistema informatico.

### Tavole di Verità

Le tavole di verità sono tabelle matematiche utilizzate nella progettazione logica e dei circuiti digitali. Aiutano a mappare la funzionalità di un circuito. Possiamo utilizzarle per aiutare a progettare circuiti digitali complessi.

Le tavole di verità hanno 1 colonna per ogni variabile di input e 1 colonna finale che mostra tutti i possibili risultati dell'operazione logica che la tavola rappresenta.

### Tipi di Circuiti Digitali

Ci sono 2 tipi di circuiti digitali: Combinatori e Sequenziali

- **Circuiti Combinatori**: In questo tipo di circuito digitale, l'output dipende dall'input che riceve in un istante. Questo tipo di circuito rimarrà costante rispetto al suo input.
- **Circuiti Sequenziali**: In questo tipo di circuito digitale, l'output dipende non solo dall'input che riceve in un istante ma anche dal precedente input che ha ricevuto. L'output generato dal precedente input viene trasferito nell'output dell'input corrente.

### Metodologia di Progettazione

Quando si progetta un circuito digitale, soprattutto complesso, è importante utilizzare strumenti dell'algebra booleana per aiutare nel processo di progettazione (esempio: mappa di Karnaugh). È anche importante suddividere tutto in circuiti più piccoli ed esaminare la tavola di verità necessaria per quel circuito più piccolo. Non cercare di affrontare tutto il circuito in una volta, suddividilo e metti gradualmente insieme i pezzi.

## [Sistemi Numerici](Number%20System/readme.md#number-systems)
I sistemi numerici sono sistemi matematici per esprimere numeri. Un sistema numerico consiste in un insieme di simboli utilizzati per rappresentare numeri e un insieme di regole per manipolare

 quei simboli. I simboli utilizzati in un sistema numerico sono chiamati numerali.

### [Tipi di Sistemi Numerici](Number%20System/readme.md#types-of-number-systems)
- [Sistema Numerico Posizionale](Number%20System/readme.md#positional-numeral-system)
- [Sistema Notazionale a Segno e Valore](Number%20System/readme.md#sign-value-notation-system)

### [Sistemi Numerici Posizionali Comuni](Number%20System/readme.md#common-positional-number-systems)
- [Binario](Number%20System/readme.md#binary)
- [Ottale](Number%20System/readme.md#octal)
- [Decimale](Number%20System/readme.md#decimal)
- [Esadecimale](Number%20System/readme.md#hexadecimal)


### Importanza del Binario
Il binario è un sistema numerico a base 2 inventato da Gottfried Leibniz composto solo da due numeri o cifre: 0 (zero) e 1 (uno). Questo sistema numerico è alla base di tutto il codice binario, utilizzato per scrivere dati digitali come le istruzioni del processore del computer utilizzate ogni giorno. Gli 0 e i 1 nel binario rappresentano rispettivamente SPENTO o ACCESO. In un transistor, uno "0" rappresenta l'assenza di flusso di elettricità, e uno "1" rappresenta che l'elettricità è permessa di fluire. In questo modo, i numeri vengono rappresentati fisicamente all'interno del dispositivo di calcolo, permettendo i calcoli.

 
Il binario è ancora il linguaggio principale per i computer ed è utilizzato con l'elettronica e l'hardware del computer per le seguenti ragioni:

- È un design semplice ed elegante.
- Il metodo 0 e 1 del binario è rapido per rilevare lo stato spento (falso) o acceso (vero) di un segnale elettrico.
- Avere solo due stati posti a distanza in un segnale elettrico lo rende meno suscettibile alle interferenze elettriche.
- I poli positivi e negativi dei supporti magnetici sono rapidamente tradotti in binario.
- Il binario è il modo più efficiente per controllare i circuiti logici.


## [Unità di Elaborazione Centrale (CPU)](CPU/readme.md#central-processing-unitcpu)
L'Unità di Elaborazione Centrale (CPU) è la parte più importante di qualsiasi computer. La CPU invia segnali per controllare le altre parti del computer, quasi come un cervello controlla un corpo. La CPU è una macchina elettronica che esegue un elenco di cose da fare del computer, chiamate istruzioni. Legge l'elenco delle istruzioni ed esegue (esegue) ciascuna in ordine. Un elenco di istruzioni che una CPU può eseguire è un programma per computer. Una CPU può elaborare più di un'istruzione alla volta su sezioni chiamate "core". Una CPU con quattro core può elaborare quattro programmi contemporaneamente. La CPU stessa è composta da tre componenti principali. Sono:
1. [Unità di Memoria o di Archiviazione](CPU/readme.md#memory-or-storage-unit)
2. [Unità di Controllo](CPU/readme.md#control-unit)
3. [Unità Aritmetica e Logica (ALU)](CPU/readme.md#arithmetic-and-logic-unit-alu)



## [Registri, Cache e RAM](/Registers%20Cache%20and%20RAM)

### [Registro](/Registers%20Cache%20and%20RAM/readme.md#register)
I registri sono piccole quantità di memoria ad alta velocità contenute all'interno della CPU. I registri sono una raccolta di "flip-flop" (un circuito utilizzato per memorizzare 1 bit di memoria). Vengono utilizzati dal processore per memorizzare piccole quantità di dati necessari durante l'elaborazione. Una CPU può avere diversi set di registri che vengono chiamati "core". I registri aiutano anche nelle operazioni aritmetiche e logiche.

Le operazioni aritmetiche sono calcoli matematici eseguiti dalla CPU su dati numerici memorizzati nei registri. Queste operazioni includono addizione, sottrazione, moltiplicazione e divisione. Le operazioni logiche sono calcoli booleani eseguiti dalla CPU su dati binari memorizzati nei registri. Queste operazioni includono confronti (ad esempio, verificare se due valori sono uguali) e operazioni logiche (ad esempio, AND, OR, NOT).

I registri sono essenziali per eseguire queste operazioni perché consentono alla CPU di accedere rapidamente e manipolare piccole quantità di dati. Memorizzando i dati frequentemente utilizzati nei registri, la CPU può evitare il processo più lento di recupero dei dati dalla memoria.

Quantità maggiori di dati possono essere memorizzate nella Cache, una memoria molto veloce situata sullo stesso circuito integrato dei registri. La cache viene utilizzata per i dati accessibili frequentemente mentre il programma è in esecuzione. Quantità ancora maggiori di dati possono essere memorizzate nella RAM. La RAM è l'acronimo di "random-access memory", un tipo di memoria che contiene dati e istruzioni che sono stati spostati dall'archiviazione su disco fino a quando il processore ne avrà bisogno.


### [Cache](/Registers%20Cache%20and%20RAM/readme.md#cache)
La memoria cache è un componente del computer basato su chip che rende più efficiente il recupero dei dati dalla memoria del computer. Funziona come un'area di archiviazione temporanea in modo che il processore del computer possa recuperare facilmente i dati. Questa area di archiviazione temporanea, nota come cache, è più facilmente disponibile per il processore rispetto alla fonte di memoria principale del computer, che è tipicamente una forma di DRAM.

La memoria cache viene talvolta chiamata memoria della CPU (unità di elaborazione centrale) perché è tipicamente integrata direttamente nel chip della CPU o posizionata su un chip separato che ha un bus di interconnessione separato con la CPU. Pertanto, è più accessibile al processore e in grado di aumentare l'efficienza perché è fisicamente vicina al processore.

Per essere vicina al processore, la memoria cache deve essere molto più piccola rispetto alla memoria principale. Di conseguenza, ha meno spazio di archiviazione. È anche più costosa della memoria principale, poiché è un chip più complesso che offre prestazioni superiori.

Ciò che sacrifica in termini di dimensioni e prezzo, lo compensa in termini di velocità. La memoria cache funziona da 10 a 100 volte più velocemente della RAM, richiedendo solo pochi nanosecondi per rispondere a una richiesta della CPU.

Il nome dell'hardware effettivo utilizzato per la memoria cache è memoria statica ad accesso casuale ad alta velocità (SRAM). Il nome dell'hardware utilizzato nella memoria principale di un computer è memoria dinamica ad accesso casuale (DRAM).

La memoria cache non deve essere confusa con il termine più ampio cache. Le cache sono archivi di dati temporanei che possono esistere sia nell'hardware che nel software. La memoria cache si riferisce al componente hardware specifico che consente ai computer di creare cache a vari livelli della rete. Una cache è un componente hardware o software utilizzato per memorizzare temporaneamente qualcosa, tipicamente dati, in un ambiente informatico.


### [RAM](/Registers%20Cache%20and%20RAM/readme.md#ram)
La RAM (memoria ad accesso casuale) è una forma di memoria del computer che può essere letta e modificata in qualsiasi ordine, tipicamente utilizzata per memorizzare dati di lavoro e codice macchina. Un dispositivo di memoria ad accesso casuale consente agli elementi di dati di essere letti o scritti in quasi lo stesso tempo indipendentemente dalla posizione fisica dei dati all'interno della memoria, a differenza di altri media di archiviazione di dati ad accesso diretto (come dischi rigidi, CD-RW, DVD-RW e le vecchie memorie a nastro e a tamburo), dove il tempo richiesto per leggere e scrivere elementi di dati varia significativamente a seconda delle loro posizioni fisiche sul supporto di registrazione, a causa di limitazioni meccaniche come la velocità di rotazione dei supporti e il movimento del braccio.


## [Istruzioni e Programma](Not-Added)
Nell'informatica, un'istruzione è una singola operazione di un processore definita dal set di istruzioni del processore. Un programma per computer è un elenco di istruzioni che dicono al computer cosa fare. Tutto ciò che un computer fa viene realizzato utilizzando un programma per computer. I programmi memorizzati nella memoria di un computer ("programmazione interna") consentono al computer di fare una cosa dopo l'altra, anche con pause tra un'operazione e l'altra.

## [Linguaggi di Programmazione](/Programming_Languages/readme.md)
Un linguaggio di programmazione è un insieme di regole che converte stringhe, o elementi grafici di programma nel caso di linguaggi di programmazione visuale, in vari tipi di output in codice macchina. I linguaggi di programmazione sono un tipo di linguaggio informatico utilizzato nella programmazione per implementare algoritmi.

I linguaggi di programmazione sono spesso divisi in due ampie categorie:
1. Il linguaggio ad alto livello utilizza una sintassi simile alla lingua inglese. Il codice sorgente viene convertito in codice macchina comprensibile tramite un compilatore o un interprete. Java e Python sono alcuni esempi di linguaggi di programmazione ad alto livello. Questi sono generalmente più lenti dei linguaggi a basso livello, ma sono più facili da utilizzare.
2. I linguaggi di programmazione a basso livello lavorano più da vicino con l'hardware e hanno maggiore controllo su di esso. Interagiscono direttamente con l'hardware. Due esempi comuni di linguaggi a basso livello sono il linguaggio macchina e il linguaggio assembly. Questi sono generalmente più veloci dei linguaggi ad alto livello, ma a costo di una maggiore difficoltà e minore leggibilità.

### [Paradigmi di Programmazione](/Programming_Languages/readme.md#Programming+Paradigms)
Esistono anche diversi *paradigmi di programmazione*. I paradigmi di programmazione sono diversi modi o stili in cui un dato programma o linguaggio di programmazione può essere organizzato. Ogni paradigma consiste di certe strutture, caratteristiche e opinioni su come affrontare i problemi di programmazione comuni.

I paradigmi di programmazione *non* sono linguaggi o strumenti. Non puoi "costruire" nulla con un paradigma. Sono più simili a un insieme di ideali e linee guida che molte persone hanno accettato, seguito e ampliato. I linguaggi di programmazione non sono sempre legati a un particolare paradigma. Ci *sono* linguaggi che sono stati costruiti con un certo paradigma in mente e hanno caratteristiche che facilitano quel tipo di programmazione più di altri (Haskell e la programmazione funzionale sono un buon esempio). Ma ci sono anche linguaggi "multi-paradigma" in cui puoi adattare il tuo codice per adattarsi a un paradigma o all'altro (JavaScript e Python sono buoni esempi).


## [Tipi di Dati](Data%20Types/readme.md#data-types)
Un tipo di dati, nella programmazione, è una classificazione che specifica quale tipo di valore ha una variabile e quale tipo di operazioni matematiche, relazionali o logiche possono essere applicate ad essa senza causare errori.

### [Tipi di Dati Primitivi](Data%20Types/readme.md#primitive-data-types)
I tipi di dati primitivi sono i tipi di dati più basilari in un linguaggio di programmazione. Sono i mattoni di costruzione dei tipi di dati più complessi. I tipi di dati primitivi sono predefiniti dal linguaggio di programmazione e sono nominati da una parola chiave riservata.

### [Tipi di Dati Primitivi Comuni](Data%20Types/readme.md#common-primitive-data-types)
- [Intero](Data%20Types/readme.md#integer)
- [Virgola Mobile](Data%20Types/readme.md#float)
- [Booleano](Data%20Types/readme.md#boolean)
- [Carattere](Data%20Types/readme.md#character)
- [Stringa](Data%20Types/readme.md#string)

### [Tipi di Dati Non Primitivi](Data%20Types/read

me.md#non-primitive-data-types)
I tipi di dati non primitivi sono anche noti come tipi di dati di riferimento. Sono creati dal programmatore e non sono definiti dal linguaggio di programmazione. I tipi di dati non primitivi sono anche chiamati tipi di dati compositi perché sono composti da altri tipi.

### [Tipi di Dati Non Primitivi Comuni](Data%20Types/readme.md#common-non-primitive-data-types)
- [Array](Data%20Types/readme.md#array)
- [Struct](Data%20Types/readme.md#struct)
- [Unione](Data%20Types/readme.md#union)
- [Puntatore](Data%20Types/readme.md#pointer)
- [Funzione](Data%20Types/readme.md#function)
- [Classe](Data%20Types/readme.md#class)

## [Istruzioni e Funzioni](Statements%20and%20Functions/readme.md)
Nella programmazione, un'istruzione è un'unità sintattica di un linguaggio di programmazione imperativo che esprime un'azione da eseguire. Un programma scritto in tale linguaggio è formato da una sequenza di una o più istruzioni. Un'istruzione può avere componenti interni (ad esempio, espressioni).
Ci sono due tipi principali di istruzioni in qualsiasi linguaggio di programmazione necessari per costruire la logica di un codice.

1. [Istruzioni Condizionali](Statements%20and%20Functions/readme.md#conditional-statements)

Esistono principalmente due tipi di istruzioni condizionali:
- if
- if-else
- switch case

2. [Cicli](Statements%20and%20Functions/readme.md#loops)

Esistono principalmente tre tipi di cicli:
- for loop
- while loop
- do-while loop (una variazione del while loop)
- do-until loop

------------

Una funzione è un blocco di istruzioni che svolge un compito specifico. Le funzioni accettano dati, li elaborano e restituiscono un risultato o eseguono un'azione. Le funzioni sono scritte principalmente per supportare il concetto di riutilizzabilità. Una volta scritta una funzione, può essere chiamata facilmente senza dover ripetere lo stesso codice.

Diversi linguaggi funzionali utilizzano sintassi diverse per scrivere le funzioni.

Leggi di più sulle funzioni [qui](Statements%20and%20Functions/readme.md#functions).


## [Strutture Dati](Data%20Structures/readme.md)
In informatica, una struttura dati è un formato per l'organizzazione, la gestione e l'archiviazione dei dati che consente un accesso e una modifica efficienti. Più precisamente, una struttura dati è una raccolta di valori di dati, le relazioni tra di essi e le funzioni o operazioni che possono essere applicate ai dati.

### Tipi di Strutture Dati
- [Array](Data%20Structures/readme.md#array)
- [Lista Collegata](Data%20Structures/readme.md#linkedlist)
- [Stack](Data%20Structures/readme.md#stack)
- [Coda](Data%20Structures/readme.md#queue)
- [Tabella Hash](Data%20Structures/readme.md#hashtable)
- [Heap](Data%20Structures/readme.md#heap)
- [Albero](Data%20Structures/readme.md#tree)
- [Grafo](Data%20Structures/readme.md#graph)

## [Algoritmi](Algorithms/readme.md)
Gli algoritmi sono insiemi di passi necessari per completare un calcolo. Sono al cuore di ciò che fanno i nostri dispositivi, e non è un concetto nuovo. Dallo sviluppo della matematica stessa, sono stati necessari algoritmi per aiutarci a completare i compiti in modo più efficiente. Oggi daremo uno sguardo a un paio di problemi di calcolo moderni, come l'ordinamento e la ricerca sui grafi, e mostreremo come li abbiamo resi più efficienti affinché tu possa trovare più facilmente voli economici o indicazioni stradali verso Winterfell o un ristorante o altro.

### [Complessità Temporale](Algorithms/Time%20Complexity/readme.md)
La complessità temporale di un algoritmo stima quanto tempo l'algoritmo impiegherà per un certo input. L'idea è di rappresentare l'efficienza come una funzione il cui parametro è la dimensione dell'input. Calcolando la complessità temporale, possiamo determinare se l'algoritmo è abbastanza veloce senza implementarlo.

### [Complessità Spaziale](Algorithms/Space%20Complexity/readme.md)
La complessità spaziale si riferisce alla quantità totale di memoria che un algoritmo/program utilizza, inclusa la memoria degli input necessari per l'esecuzione. Calcolare lo spazio occupato dalle variabili in un algoritmo/program permette di determinare la complessità spaziale.

### [Ordinamento](Algorithms/Sorting/readme.md)
L'ordinamento è il processo di disposizione di un elenco di elementi in un ordine particolare. Ad esempio, se avessi un elenco di nomi, potresti volerli ordinare in ordine alfabetico. In alternativa, se avessi un elenco di numeri, potresti volerli mettere in ordine dal più piccolo al più grande. L'ordinamento è un compito comune, e possiamo farlo in molti modi diversi.

### [Ricerca](Algorithms/Searching/readme.md)
La ricerca è un algoritmo per trovare un certo elemento target all'interno di un contenitore. Gli algoritmi di ricerca sono progettati per verificare la presenza di un elemento o recuperare un elemento da qualsiasi struttura dati in cui è archiviato.

### [Algoritmi Basati su Stringhe](Algorithms/String%20Based%20Algorithms/readme.md)
Le stringhe sono una delle strutture dati più utilizzate e importanti nella programmazione. Questo repository contiene alcuni degli algoritmi più utilizzati che aiutano a migliorare il tempo di ricerca, ottimizzando il nostro codice.

### [Ricerca sui Grafi](Algorithms/Graph/readme.md)
La ricerca sui grafi è il processo di ricerca attraverso un grafo per trovare un particolare nodo. Un grafo è una struttura dati che consiste in un insieme finito (e possibilmente mutabile) di vertici o nodi o punti, insieme a un insieme di coppie non ordinate di questi vertici per un grafo non orientato o un insieme di coppie ordinate per un grafo orientato. Queste coppie sono conosciute come archi, archi orientati o linee per un grafo non orientato e come frecce, archi diretti, archi diretti o linee dirette per un grafo orientato. I vertici possono far parte della struttura del grafo o possono essere entità esterne rappresentate da indici interi o riferimenti. I grafi sono una delle strutture dati più utili per molte applicazioni nel mondo reale. I grafi sono utilizzati per modellare relazioni a coppie tra oggetti. Ad esempio, la rete delle rotte aeree è un grafo in cui le città sono i vertici e le rotte dei voli sono gli archi. I grafi sono anche usati per rappresentare reti. Internet può essere modellato come un grafo in cui i computer sono i vertici e i collegamenti tra i computer sono gli archi. I grafi sono anche utilizzati nei social network come LinkedIn e Facebook. I grafi sono utilizzati per rappresentare molte applicazioni del mondo reale: reti informatiche, progettazione di circuiti, e programmazione aeronautica, solo per citarne alcune.

### [Programmazione Dinamica](Algorithms/Dynamic%20Programming/README.md)
La programmazione dinamica è sia un metodo di ottimizzazione matematica che un metodo di programmazione informatica. Richard Bellman sviluppò il metodo negli anni '50 e ha trovato applicazioni in numerosi campi, dall'ingegneria aerospaziale all'economia. In entrambi i contesti, si riferisce a semplificare un problema complicato suddividendolo in sottoproblemi più semplici in modo ricorsivo. Anche se alcuni problemi decisionali non possono essere scomposti in questo modo, le decisioni che si estendono su più punti nel tempo spesso si scompongono ricorsivamente. Allo stesso modo, in informatica, se un problema può essere risolto in modo ottimale suddividendolo in sottoproblemi e poi trovando ricorsivamente le soluzioni ottimali ai sottoproblemi, allora si dice che ha una struttura ottimale. La programmazione dinamica è uno dei modi per risolvere problemi con queste proprietà. Il processo di suddivisione di un problema complicato in sottoproblemi più semplici è chiamato "divide et impera".

### [Algoritmi Greedy](Algorithms/Greedy%20Algorithm/readme.md)
Gli algoritmi greedy sono una classe di algoritmi semplice e intuitiva che può essere utilizzata per trovare la soluzione ottimale a problemi di ottimizzazione. Sono chiamati greedy perché, a ogni passo, fanno la scelta che sembra migliore in quel momento. Questo significa che gli algoritmi greedy non garantiscono di restituire la soluzione ottimale globale, ma invece fanno scelte localmente ottimali nella speranza di trovare un ottimo globale. Gli algoritmi greedy sono utilizzati per problemi di ottimizzazione. Un problema di ottimizzazione può essere risolto utilizzando Greedy se il problema ha la seguente proprietà: a ogni passo, possiamo fare una scelta che sembra migliore al momento e ottenere la soluzione ottimale al problema completo.

### [Backtracking](Algorithms/Backtracking/README.md)
Il backtracking è una tecnica algoritmica per risolvere problemi in modo ricorsivo cercando di costruire una soluzione incrementale, un pezzo alla volta, rimuovendo quelle soluzioni che non soddisfano i vincoli del problema in qualsiasi punto del tempo (il tempo qui si riferisce al tempo trascorso fino a raggiungere qualsiasi livello dell'albero di ricerca).

### [Branch and Bound](Algorithms/Branch%20and%20Bound/README.md)
Branch and Bound è una tecnica generale per risolvere problemi di ottimizzazione combinatoria. È una tecnica di enumerazione sistematica che riduce il numero di soluzioni candidate utilizzando la struttura del problema per eliminare soluzioni candidate che non possono essere ottimali.

### [Complessità Temporale e Complessità Spaziale di Diversi Algoritmi di Ricerca e Ordinamento](Not-Added)
**Complessità Temporale**: È definita come il numero di volte che un particolare set di istruzioni si prevede venga eseguito piuttosto che il tempo totale impiegato. Poiché il tempo è un fenomeno dipendente, la complessità temporale può variare in base ad alcuni fattori esterni come la velocità del processore, il compilatore utilizzato, ecc.

**Complessità Spaziale**: È lo spazio totale di memoria consumato dal programma per la sua esecuzione.

Entrambi sono calcolati come funzione della dimensione dell'input (n). La complessità temporale di un algoritmo è espressa in notazione big O.

L'efficienza di un algoritmo dipende da questi due parametri.

### Tipi di Complessità Temporale:

- **Complessità Temporale nel Migliore dei Casi**: L'input per cui l'algoritmo impiega meno tempo o il tempo minimo. Nel migliore dei casi, calcoliamo la complessità temporale del limite inferiore di un algoritmo. Ad esempio: se i dati da cercare sono presenti nella prima posizione di un grande array di dati in una ricerca lineare, allora si verifica il migliore dei casi.

- **Complessità Temporale Media**: Prendiamo tutti gli input casuali e calcoliamo il tempo di elaborazione per tutti gli input. Poi, lo dividiamo per il numero totale di input.

- **Complessità Temporale nel Peggiore dei Casi**: Definisce l'input per cui l'algoritmo impiega più tempo o il tempo massimo. Nel peggiore dei casi, calcoliamo il limite superiore di un algoritmo. Esempio: Se i dati da cercare sono presenti nell'ultima posizione di un grande array di dati in un algoritmo di ricerca lineare, allora si verifica il peggiore dei casi.

Alcune complessità temporali comuni sono:

- **O(1)**: Questo denota il tempo costante. O(1) di solito significa che un algoritmo avrà un tempo costante indipendentemente dalla dimensione dell'input. Gli Hash Map sono perfetti esempi di tempo costante.

- **O(log n)**: Questo denota il tempo logaritmico. O(log n) significa che diminuisce a ogni iterazione per le operazioni. Cercare elementi negli Alberi di Ricerca Binari (BST) è un buon esempio di tempo logaritmico.

- **O(n)**: Questo denota il tempo lineare. O(n) significa che le prestazioni sono direttamente proporzionali alla dimensione dell'input. In termini semplici, il numero di input e il tempo impiegato per eseguire quegli input saranno proporzionali. La ricerca lineare negli array è un ottimo esempio di complessità temporale lineare.

- **O(n\*n)**: Questo denota il tempo quadratico. O(n^2) significa che le prestazioni sono direttamente proporzionali al quadrato dell'input preso. In modo semplice, il tempo impiegato per l'esecuzione richiederà grosso modo il quadrato della dimensione dell'input. I cicli annidati sono perfetti esempi di complessità temporale quadratica.

- **O(n log n)**: Questo denota la complessità temporale polinomiale. O(n log n) significa che le prestazioni sono n volte quelle di O(log n), (che è la complessità nel peggiore dei casi). Un buon esempio sarebbe un algoritmo di divide et impera come il merge sort. Questo algoritmo prima divide l'insieme, che richiede O(log n) tempo, poi affronta e ordina l'insieme, che richiede O(n) tempo; quindi, il merge sort richiede O(n log n) tempo.

| Algoritmo	     |             |  Complessità Temporale |	       | Complessità Spaziale |
|   :---:        |  :---:      |  :---: 	   |   :---: 	   |   :---:          |
|  	             | Migliore	   | Media	         |  Peggiore	     |  Peggiore         |
| Selection Sort | Ω(n^2)	     | θ(n^2)	     | O(n^2)	       | O(1)             |
| Bubble Sort	 | Ω(n)	       | θ(n^2)	     | O(n^2)	       | O(1)             |
| Insertion Sort | Ω(n)	       | θ(n^2)	     | O(n^2)	       | O(1)             |
| Heap Sort	     | Ω(n log(n)) | θ(n log(n)) | O(n log(n)) | O(1)             |
| Quick Sort	   | Ω(n log(n)) | θ(n log(n)) | O(n^2)	     | O(n)             |
| Merge Sort	   | Ω(n log(n)) | θ(n log(n)) | O(n log(n)) | O(n)             |
| Bucket Sort    | Ω(n +k)	   | θ(n +k)	   | O(n^2)	     | O(n)             |
| Radix Sort  	 | Ω(nk)	     | θ(nk)	     | O(nk)	       | O(n + k)         |
| Count Sort  	 | Ω(n +k)	   | θ(n +k)	   | O(n +k)	     | O(k)             |
| Shell Sort  	 | Ω(n log(n)) | θ(n log(n)) | O(n^2)	     | O(1)             |
| Tim Sort	     | Ω(n)	       | θ(n log(n)) | O(n log(n)) | O(n)             |
| Tree Sort   	 | Ω(n log(n)) | θ(n log(n)) | O(n^2)	     | O(n)             |
| Cube Sort	     | Ω(n)	       | θ(n log(n)) | O(n log(n)) | O(n)             |

| Algoritmo	     |             |  Complessità Temporale |	     |
|   :---:        |  :---:      |  :---: 	 |   :---: 	   |  
|  	             | Migliore	   | Media	   |  Peggiore	     |
| Linear Search  | O(1)	       | O(N)	     | O(N)	         | O(1)  |
| Binary Search	 | O(1)	       | O(logN)   | O(logN)	     |


## [Alan Turing](Not-Added)
Alan Turing (nato il 23 giugno 1912, Londra, Inghilterra – morto il 7 giugno 1954, Wilmslow, Cheshire) è stato un matematico e logico inglese. Ha studiato presso l'Università di Cambridge e l'Istituto per gli Studi Avanzati di Princeton. Nel suo influente articolo del 1936 "On Computable Numbers", dimostrò che non poteva esistere alcun metodo algoritmico universale per determinare la verità in matematica e che la matematica conterrà sempre proposizioni indecidibili (a differenza delle proposizioni sconosciute). Quel documento introdusse anche la macchina di Turing. Credeva che i computer sarebbero stati capaci di pensiero indistinguibile da quello umano e propose un semplice test (vedi Turing test) per valutare questa capacità. I suoi articoli sull'argomento sono ampiamente riconosciuti come le fondamenta della ricerca sull'intelligenza artificiale. Fece un lavoro prezioso nella crittografia durante la Seconda Guerra Mondiale, svolgendo un ruolo importante nella decodifica del codice Enigma utilizzato dalla Germania per le comunicazioni radio. Dopo la guerra, insegnò presso l'Università di Manchester e iniziò a lavorare su quello che oggi è conosciuto come intelligenza artificiale. In mezzo a questo lavoro rivoluzionario, Turing fu trovato morto nel suo letto, avvelenato dal cianuro. La sua morte seguì il suo arresto per un atto omosessuale (all'epoca un reato) e la condanna a 12 mesi di terapia ormonale.

A seguito di una campagna pubblica nel 2009, il Primo Ministro britannico Gordon Brown fece una scusa pubblica ufficiale a nome del governo britannico per il trattamento orribile riservato a Turing. La Regina Elisabetta II concesse un perdono postumo nel 2013. Il termine "Alan Turing law" è ora usato informalmente per riferirsi a una legge del 2017 nel Regno Unito che ha concesso il perdono retroattivo agli uomini ammoniti o condannati sotto la legislazione storica che vietava gli atti omosessuali.

Turing ha un'estesa eredità con statue di lui e molte cose a lui intitolate, tra cui un premio annuale per le innovazioni in informatica. Appare sulla banconota da £50 della Bank of England, rilasciata il 23 giugno 2021, in coincidenza con il suo compleanno. Una serie della BBC del 2019, votata dal pubblico, lo ha nominato la più grande persona del 20° secolo.


## [Ingegneria del Software](Software%20Engineering/readme.md)
L'ingegneria del software è il ramo dell'informatica che si occupa della progettazione, dello sviluppo, del collaudo e della manutenzione delle applicazioni software. Gli ingegneri del software applicano i principi dell'ingegneria e la conoscenza dei linguaggi di programmazione per creare soluzioni software per gli utenti finali.

Vediamo le varie definizioni di ingegneria del software:

- IEEE, nel suo standard 610.12-1990, definisce l'ingegneria del software come l'applicazione di un approccio sistematico, disciplinato e calcolabile per lo sviluppo, l'operazione e la manutenzione del software.
- Fritz Bauer la definisce come "l'istituzione e l'uso di principi ingegneristici standard. Aiuta a ottenere software economico, affidabile e che funziona

 in modo efficiente su macchine reali."
- Boehm definisce l'ingegneria del software come "l'applicazione pratica della conoscenza scientifica alla progettazione creativa e alla costruzione di programmi per computer. Include anche la documentazione associata necessaria per svilupparli, gestirli e mantenerli."

### Compiti e responsabilità dell'ingegnere del software
Gli ingegneri di successo sanno come utilizzare i giusti linguaggi di programmazione, piattaforme e architetture per sviluppare tutto, dai giochi per computer ai sistemi di controllo della rete. Oltre a costruire i propri sistemi, gli ingegneri del software testano, migliorano e mantengono anche il software sviluppato da altri ingegneri.

In questo ruolo, le tue attività quotidiane potrebbero includere:

- Progettare e mantenere sistemi software
- Valutare e testare nuovi programmi software
- Ottimizzare il software per velocità e scalabilità
- Scrivere e testare codice
- Consultarsi con clienti, ingegneri, specialisti della sicurezza e altre parti interessate
- Presentare nuove funzionalità alle parti interessate e ai clienti interni

### Fasi dell'Ingegneria del Software
Il processo di ingegneria del software comprende diverse fasi, tra cui la raccolta dei requisiti, la progettazione, l'implementazione, il collaudo e la manutenzione. Seguendo un approccio disciplinato allo sviluppo del software, gli ingegneri del software possono creare software di alta qualità che soddisfa le esigenze degli utenti.

- La prima fase dell'ingegneria del software è la raccolta dei requisiti. In questa fase, l'ingegnere del software lavora con il cliente per determinare i requisiti funzionali e non funzionali del software. I requisiti funzionali descrivono cosa dovrebbe fare il software, mentre i requisiti non funzionali descrivono come dovrebbe farlo. La raccolta dei requisiti è una fase critica, poiché pone le basi per l'intero processo di sviluppo del software.

- Dopo la raccolta dei requisiti, la fase successiva è la progettazione. In questa fase, l'ingegnere del software crea un piano dettagliato per l'architettura e la funzionalità del software. Questo piano include un documento di progettazione del software che specifica la struttura, il comportamento e le interazioni del software con altri sistemi. Il documento di progettazione del software è essenziale poiché funge da guida per la fase di implementazione.

- La fase di implementazione è dove l'ingegnere del software crea il codice effettivo per il software. Questo è il momento in cui il documento di progettazione viene trasformato in software funzionante. La fase di implementazione comprende la scrittura del codice, la compilazione e il collaudo per garantire che soddisfi i requisiti specificati nel documento di progettazione.

- Il collaudo è una fase critica nell'ingegneria del software. In questa fase, l'ingegnere del software verifica che il software funzioni correttamente, sia affidabile e facile da usare. Questo coinvolge diversi tipi di collaudo, tra cui collaudo unitario, collaudo di integrazione e collaudo del sistema. Il collaudo assicura che il software soddisfi i requisiti e funzioni come previsto.

- L'ultima fase dell'ingegneria del software è la manutenzione. In questa fase, l'ingegnere del software apporta modifiche al software per correggere errori, aggiungere nuove funzionalità o migliorare le prestazioni. La manutenzione è un processo continuo che continua per tutta la durata del software.

### Perché l'Ingegneria del Software è Popolare?

- **Informatica**: Fornisce le basi scientifiche per il software così come l'ingegneria elettrica dipende principalmente dalla fisica.
- **Scienze Gestionali**: L'ingegneria del software è intensiva dal punto di vista del lavoro e richiede controllo tecnico e manageriale. Pertanto, è ampiamente utilizzata nelle scienze gestionali.
- **Economia**: In questo settore, l'ingegneria del software aiuta a stimare le risorse e controllare i costi. Un sistema informatico deve essere sviluppato e i dati devono essere mantenuti regolarmente all'interno di un budget definito.
- **Ingegneria dei Sistemi**: La maggior parte del software è un componente di un sistema molto più grande. Ad esempio, il software in un sistema di monitoraggio industriale o il software di volo su un aereo. I metodi di ingegneria del software dovrebbero essere applicati allo studio di questo tipo di sistema.

Ecco la traduzione del testo dall'inglese all'italiano:

---

## [Data Science](Data%20Science/readme.md)

La Data Science estrae informazioni preziose dai dati spesso disordinati applicando informatica, statistica e conoscenza del dominio in considerazione. Esempi di utilizzo della data science includono la derivazione del sentimento dei clienti dai record delle chiamate o i sistemi di raccomandazione basati sui dati di vendita.

## [Circuiti Integrati](Integrated%20Circuits/readme.md)

Un circuito integrato o circuito integrato monolitico (indicato anche come IC, chip o microchip) è un insieme di circuiti elettronici su un piccolo pezzo piatto (o "chip") di materiale semiconduttore, di solito silicio. Moltissimi piccoli MOSFET (transistor a effetto di campo metallo-ossido-semiconduttore) vengono integrati in un piccolo chip. Ciò si traduce in circuiti che sono ordini di grandezza più piccoli, più veloci e meno costosi di quelli costruiti con componenti elettronici discreti. La capacità di produzione di massa degli IC, la loro affidabilità e l'approccio modulare alla progettazione di circuiti integrati hanno assicurato la rapida adozione degli IC standardizzati al posto dei transistor discreti. Gli IC sono ora utilizzati in quasi tutte le apparecchiature elettroniche e hanno rivoluzionato il mondo dell'elettronica. Computer, telefoni cellulari e altri elettrodomestici sono ormai parti inseparabili della struttura delle società moderne, resi possibili dalle piccole dimensioni e dal basso costo degli IC come i moderni processori per computer e microcontrollori.

L'integrazione su larga scala è diventata pratica grazie ai progressi tecnologici nella fabbricazione di dispositivi semiconduttori MOS. Dai loro esordi negli anni '60, la dimensione, la velocità e la capacità dei chip sono progredite enormemente, grazie agli avanzamenti tecnici che consentono di inserire sempre più transistor MOS su chip della stessa dimensione: un chip moderno può avere molti miliardi di transistor MOS in un'area delle dimensioni di un'unghia umana. Questi progressi, seguendo approssimativamente la legge di Moore, fanno sì che i chip di oggi abbiano milioni di volte la capacità e migliaia di volte la velocità dei chip dei primi anni '70.

Gli IC hanno due principali vantaggi rispetto ai circuiti discreti: costo e prestazioni. Il costo è basso perché i chip, con tutti i loro componenti, sono stampati come un'unità tramite fotolitografia anziché essere costruiti un transistor alla volta. Inoltre, i circuiti integrati confezionati utilizzano molto meno materiale rispetto ai circuiti discreti. Le prestazioni sono elevate perché i componenti dell'IC commutano rapidamente e consumano relativamente poca energia grazie alle loro piccole dimensioni e alla loro vicinanza. Lo svantaggio principale degli IC è l'elevato costo di progettazione e fabbricazione delle maschere fotolitografiche richieste. Questo alto costo iniziale rende gli IC commercialmente viabili solo quando si prevede una produzione di grandi volumi.

### Tipi

I distributori di componenti elettronici moderni spesso suddividono ulteriormente i circuiti integrati:

- I circuiti integrati digitali sono categorizzati come IC logici (come microprocessori e microcontrollori), chip di memoria (come memoria MOS e memoria a gate flottante), IC di interfaccia (shifter di livello, serializer/deserializer, ecc.), IC di gestione dell'alimentazione e dispositivi programmabili.
- I circuiti integrati analogici sono categorizzati come circuiti integrati lineari e circuiti a radiofrequenza (circuiti RF).
- I circuiti integrati a segnali misti sono categorizzati come IC di acquisizione dati (convertitori A/D, convertitori D/A e potenziometri digitali), IC di temporizzazione/clock, circuiti a capacità commutata (SC) e circuiti CMOS RF.
- I circuiti integrati tridimensionali (IC 3D) sono categorizzati in IC con via attraverso il silicio (TSV) e IC con connessione Cu-Cu.


## [Programmazione Orientata agli Oggetti](Object%20Oriented%20Programming/readme.md)

La programmazione orientata agli oggetti è un paradigma di programmazione fondamentale basato sui concetti di oggetti e dati.

È il modo standard di scrivere codice che ogni programmatore deve seguire per garantire una migliore leggibilità e riutilizzabilità del codice.

### * Ci sono quattro concetti base della programmazione orientata agli oggetti:
- Astrazione
- Incapsulamento
- Ereditarietà
- Polimorfismo

Leggi di più su questi concetti della programmazione orientata agli oggetti [qui](Object%20Oriented%20Programming/readme.md)



## [Programmazione Funzionale](Functional%20Programming/readme.md)

In informatica, la programmazione funzionale è un paradigma di programmazione in cui i programmi sono costruiti applicando e componendo funzioni. È un paradigma di programmazione dichiarativa in cui le definizioni delle funzioni sono alberi di espressioni che mappano valori su altri valori, piuttosto che una sequenza di istruzioni imperative che aggiornano lo stato del programma in esecuzione.

Nella programmazione funzionale, le funzioni sono trattate come cittadini di prima classe, il che significa che possono essere legate a nomi (compresi identificatori locali), passate come argomenti e restituite da altre funzioni, proprio come qualsiasi altro tipo di dato. Questo consente di scrivere programmi in uno stile dichiarativo e composabile, dove piccole funzioni sono combinate in modo modulare.

La programmazione funzionale è talvolta trattata come sinonimo di programmazione puramente funzionale, un sottoinsieme della programmazione funzionale che tratta tutte le funzioni come funzioni matematiche deterministiche, o funzioni pure. Quando una funzione pura è chiamata con determinati argomenti, restituirà sempre lo stesso risultato e non può essere influenzata da alcuno stato mutabile o altri effetti collaterali. Questo contrasta con le procedure impure, comuni nella programmazione imperativa, che possono avere effetti collaterali (come modificare lo stato del programma o prendere input da un utente). I sostenitori della programmazione puramente funzionale affermano che, limitando gli effetti collaterali, i programmi possono avere meno bug, essere più facili da eseguire il debug e testare, e essere più adatti alle procedure di verifica formale.

La programmazione funzionale ha le sue radici nell'accademia, evolvendosi dal calcolo lambda, un sistema formale di calcolo basato solo su funzioni. La programmazione funzionale è storicamente stata meno popolare rispetto alla programmazione imperativa, ma molti linguaggi funzionali sono oggi utilizzati nell'industria e nell'istruzione.

Alcuni esempi di linguaggi di programmazione funzionale sono:
- <a href="https://lisp-lang.org/"> Common Lisp </a>
- <a href="https://www.scheme.org/"> Scheme </a>
- <a href="https://racket-lang.org/"> Racket </a>
- <a href="https://www.erlang.org/"> Erlang </a>
- <a href="https://www.haskell.org/"> Haskell </a>
- <a href="https://fsharp.org/"> F# </a>
- <a href="https://cs.lmu.edu/~ray/notes/introml/"> ML </a>

La programmazione funzionale è derivata storicamente dal *calcolo lambda*. Il calcolo lambda è un framework sviluppato da Alonzo Church per studiare i calcoli con le funzioni. È spesso chiamato "il linguaggio di programmazione più piccolo del mondo." Fornisce una definizione di ciò che è computabile e ciò che non lo è. È equivalente a una macchina di Turing nella sua capacità computazionale e qualsiasi cosa computabile dal calcolo lambda, proprio come qualsiasi cosa computabile da una macchina di Turing, è computabile. Fornisce un framework teorico per descrivere le funzioni e le loro valutazioni.

Alcuni concetti essenziali della programmazione funzionale sono:
- Funzioni pure
- Ricorsione
- Trasparenza referenziale
- Funzioni come prime classi e funzioni di ordine superiore
- Le variabili sono immutabili.

**Funzioni pure**: Queste funzioni hanno due proprietà principali. Prima, producono sempre lo stesso output per gli stessi argomenti indipendentemente da qualsiasi altra cosa. In secondo luogo, non hanno effetti collaterali, ovvero non modificano argomenti o variabili locali/globali o flussi di input/output. Questa ultima proprietà è chiamata *immutabilità*. L'unico risultato di una funzione pura è il valore che restituisce. Sono deterministiche. I programmi scritti usando la programmazione funzionale sono facili da eseguire il debug perché non hanno effetti collaterali o I/O nascosto. Le funzioni pure rendono anche più facile scrivere applicazioni parallele/concurrenti. Quando il codice è scritto in questo stile, un compilatore intelligente può fare molte cose: può parallelizzare le istruzioni, attendere di valutare i risultati fino a quando sono necessari e memorizzare i risultati poiché i risultati non cambiano finché l'input non cambia. Ecco un semplice esempio di una funzione pura in Python:

```python
def sum(x ,y): # sum è una funzione che prende x e y come argomenti
    return x + y  # restituisce x + y senza cambiare il valore
```

**Ricorsione**: Non ci sono loop "for" o "while" nei linguaggi di programmazione funzionale pura. L'iterazione è implementata attraverso la ricorsione. Le funzioni ricorsive chiamano ripetutamente se stesse fino a quando viene raggiunto un caso base. Ecco un semplice esempio di una funzione ricorsiva in C:

```c
int fib(n) {
  if(n <= 1)
    return 1;
   else
     return (fib(n-1) + fib(n-2));
}
```

**Trasparenza referenziale**: Nei programmi funzionali, le variabili una volta definite non cambiano il loro valore durante l'esecuzione del programma. I programmi funzionali non hanno istruzioni di assegnamento. Se dobbiamo memorizzare un valore, definiamo una nuova variabile. Questo elimina qualsiasi possibilità di effetti collaterali perché qualsiasi variabile può essere sostituita con il suo valore effettivo in qualsiasi punto dell'esecuzione. Lo stato di qualsiasi variabile è costante in ogni istante. Esempio:

```bash
x = x + 1 # questo ha cambiato il valore assegnato alla variabile x
         # Pertanto, l'espressione NON è referenzialmente trasparente
```

**Le funzioni sono di prima classe e possono essere di ordine superiore**: Le funzioni di prima classe sono trattate come variabili di prima classe. Le variabili di prima classe possono essere passate alle funzioni come parametri, possono essere restituite dalle funzioni o memorizzate in strutture dati.

Una combinazione di applicazioni di funzioni può essere definita usando una forma LISP chiamata **funcall**, che prende come argomenti una funzione e una serie di argomenti e applica quella funzione a quegli argomenti:

```Lisp
(defun filter (list-of-elements test)
    (cond ((null list-of-elements) nil)
          ((funcall test (car list-of-elements))
            (cons (car list-of-elements)
                (filter (cdr list-of-elements)
                      test)))
           (t (filter (cdr list-of-elements)
                       test))))
```

La funzione **filter** applica il test al primo elemento della lista. Se il test restituisce non-nil, consente l'elemento al risultato di filter applicato al cdr della lista; altrimenti, restituisce semplicemente il cdr filtrato. Questa funzione può essere utilizzata con diversi predicati passati come parametri per eseguire una varietà di compiti di filtraggio:

```Lisp
    > (filter '(1 3 -9 5 -2 -7 6) #'plusp)   ; filtra tutti i numeri negativi
```    
    output: (1 3 5 6)

```Lisp
   > (filter '(1 2 3 4 5 6 7 8 9) #'evenp)   ; filtra tutti i numeri dispari
```   
   output: (2 4 6 8)

   e così via.

**Le variabili sono immutabili**: Nella programmazione funzionale, non possiamo modificare una variabile dopo che è stata inizializzata. Possiamo creare nuove variabili, ma non possiamo modificare quelle esistenti, e questo aiuta a mantenere lo stato durante l'esecuzione di un programma. Una volta creata una variabile e impostato il suo valore, possiamo avere piena fiducia sapendo che il valore di quella variabile non cambierà mai.


## [Sistemi Operativi](Operating%20Systems/readme.md)

Un sistema operativo (o OS in breve) funge da intermediario tra un utente del computer e l'hardware del computer. Lo scopo di un sistema operativo è fornire un ambiente in cui un utente possa eseguire programmi in modo conveniente ed efficiente.

Un sistema operativo è un software che gestisce l'hardware del computer. L'hardware deve fornire meccanismi appropriati per garantire il corretto funzionamento del sistema informatico e per prevenire che i programmi degli utenti interferiscano con il funzionamento corretto del sistema.

Una definizione ancora più comune è che il sistema operativo è l'unico programma che è sempre in esecuzione sul computer (di solito chiamato kernel), mentre tutto il resto sono programmi applicativi.

I sistemi operativi possono essere visti da due punti di vista: come gestori delle risorse e come macchine estese. Dal punto di vista del gestore delle risorse, il compito del sistema operativo è gestire le diverse parti del sistema in modo efficiente. Dal punto di vista della macchina estesa, il compito del sistema è fornire agli utenti astrazioni che sono più comode da usare rispetto alla macchina reale. Queste includono processi, spazi di indirizzamento e file.

I sistemi operativi hanno una lunga storia, dall'epoca in cui hanno sostituito l'operatore ai moderni sistemi di multiprogrammazione. I momenti salienti includono i primi sistemi batch, i sistemi di multiprogrammazione e i sistemi per computer personali.

Poiché i sistemi operativi interagiscono strettamente con l'hardware, una certa conoscenza dell'hardware del computer è utile per comprenderli. I computer sono costituiti da processori, memorie e dispositivi di I/O. Queste parti sono collegate tramite bus.

I concetti di base su cui sono costruiti tutti i sistemi operativi sono processi, gestione della memoria, gestione degli I/O, sistema dei file e sicurezza. Il cuore di qualsiasi sistema operativo è il set di chiamate di sistema che può gestire. Queste indicano cosa fa il sistema operativo.

### Sistema operativo come gestore delle risorse

Il sistema operativo gestisce tutti i pezzi di un sistema complesso. I computer moderni sono costituiti da processori, memorie, timer, dischi, mouse, interfacce di rete, stampanti e una vasta gamma di altri dispositivi. 

Dal punto di vista dal basso verso l'alto, il compito del sistema operativo è fornire una allocazione ordinata e controllata dei processori, delle memorie e dei dispositivi di I/O tra i vari programmi che ne hanno bisogno.

I moderni sistemi operativi permettono a più programmi di essere in memoria ed eseguiti simultaneamente. Immagina cosa succederebbe se tre programmi in esecuzione su un computer tentassero di stampare il loro output simultaneamente sulla stessa stampante. Il risultato sarebbe un completo caos. Il sistema operativo può portare ordine al potenziale caos memorizzando tutto l'output destinato alla stampante sul disco. Quando un programma è terminato, il sistema operativo può quindi copiare il suo output dal file sul disco per la stampante, mentre allo stesso tempo, l'altro programma può continuare a generare altro output, ignaro del fatto che l'output non sta andando ancora alla stampante.

Quando un computer (o una rete) ha più di un utente, è necessario gestire e proteggere la memoria, i dispositivi di I/O e altre risorse ancora di più, poiché gli utenti potrebbero altrimenti interferire tra loro. Inoltre, gli utenti spesso devono condividere non solo l'hardware ma anche informazioni (file, database, ecc.). In breve, questo punto di vista del sistema operativo sostiene che il suo compito principale è tenere traccia di quali programmi stanno usando quale risorsa, concedere le richieste di risorse, contabilizzare l'uso e mediare le richieste conflittuali tra diversi programmi e utenti.

### Sistema operativo come macchina estesa

L'architettura della maggior parte dei computer a livello di linguaggio macchina è primitiva e scomoda da programmare, specialmente per l'input/output. Per rendere questo punto più concreto, considera i moderni dischi rigidi SATA (Serial ATA) utilizzati nella maggior parte dei computer. Cosa dovrebbe sapere un programmatore per utilizzare il disco. Da allora, l'interfaccia è stata rivista più volte ed è più complicata rispetto al 2007. Nessun programmatore sano di mente vorrebbe trattare con questo disco a livello hardware. Invece, un pezzo di software chiamato driver del disco si occupa dell'hardware e fornisce un'interfaccia per leggere e scrivere blocchi del disco, senza entrare nei dettagli.

I sistemi operativi contengono molti driver per il controllo dei dispositivi di I/O. Ma anche questo livello è molto troppo basso per la maggior parte delle applicazioni. Per questo motivo, tutti i sistemi operativi forniscono un ulteriore livello di astrazione per l'utilizzo dei dischi: i file. Utilizzando questa astrazione, i programmi possono creare, scrivere e leggere file senza dover affrontare i dettagli disordinati di come funziona l'hardware. Questa astrazione è la chiave per gestire tutta questa complessità. Buone astrazioni trasformano un compito quasi impossibile in due compiti gestibili. Il primo è definire e implementare le astrazioni. Il secondo è utilizzare queste astrazioni per risolvere il problema in questione.

### Storia dei Sistemi Operativi

- **Prima Generazione (1945-55)**: Poco progresso è stato raggiunto nella costruzione di computer digitali dopo i disastrosi tentativi di Babbage fino all'era della Seconda Guerra Mondiale. Alla Iowa State University, il professor John Atanasoff e il suo studente Clifford Berry crearono quello che oggi è riconosciuto come il primo computer digitale operativo. Konrad Zuse a Berlino costruì il computer Z3 utilizzando relè elettromeccanici quasi nello stesso periodo. Il Mark I fu creato da Howard Aiken ad Harvard, il Colossus da un team di scienziati a Bletchley Park in Inghilterra, e l'ENIAC da William Mauchley e il suo dottorando J. Presper Eckert all'Università della Pennsylvania nel 1944.

- **Seconda Generazione (1955-65)**: L'invenzione del transistor a metà degli anni '50 cambiò drasticamente la situazione. I computer divennero abbastanza affidabili da poter essere fabbricati e venduti ai clienti paganti con l'assunzione che avrebbero continuato a funzionare abbastanza a lungo per eseguire un lavoro significativo. I mainframe, come sono ora conosciuti, erano tenuti chiusi in enormi sale computer particolarmente climatizzate, con squadre di operatori qualificati per gestirli. Solo grandi aziende, importanti enti governativi o istituzioni potevano permettersi il prezzo di diversi milioni di dollari.

- **Terza Generazione (1965-80)**: Rispetto ai computer di seconda generazione, costruiti con transistor singoli, l'IBM 360 fu la prima grande linea di computer a utilizzare circuiti integrati (IC) (in piccola scala). Di conseguenza, offrì un notevole vantaggio in termini di prezzo/performance. Fu un successo immediato e tutti gli altri grandi produttori abbracciarono rapidamente il concetto di una famiglia di computer interoperabili. Tutto il software, incluso il sistema operativo OS/360, doveva essere compatibile con tutti i modelli nel design originale. Doveva funzionare su sistemi enormi, che spesso sostituivano i 7094 per calcoli pesanti e previsioni meteorologiche, e su sistemi piccoli, che spesso sostituivano solo i 1401 per trasferimenti di schede a nastro. Entrambi i sistemi con pochi periferici e i sistemi con molti periferici dovevano funzionare bene con esso. Doveva funzionare sia in ambienti professionali che accademici. Soprattutto, doveva essere efficace per ciascuna di queste numerose applicazioni.

- **Quarta Generazione (1980-Presente)**: L'era dei computer personali iniziò con la creazione di circuiti LSI (Large Scale Integration), processori con migliaia di transistor su un centimetro quadrato di silicio. Anche se i computer personali, originariamente conosciuti come microcomputer, non cambiarono significativamente in architettura rispetto ai minicomputer della classe PDP-11, differivano significativamente nel prezzo.

- **Quinta Generazione (1990-Presente)**: Le persone hanno desiderato un dispositivo di comunicazione portatile sin da quando il detective Dick Tracy nella striscia comica degli anni '40 iniziò a conversare con il suo "orologio radio a due vie". Nel 1946, un vero telefono mobile fece il suo debutto e pesava circa 40 chilogrammi. Il primo vero telefono portatile fece il suo debutto negli anni '70 ed era incredibilmente leggero, circa un chilogrammo. Era scherzosamente chiamato "il mattone". Presto, tutti stavano cercando uno.

### Funzioni di un OS

- **Convenienza**: Un OS rende un computer più conveniente da usare

.
- **Efficienza**: Un OS permette di utilizzare le risorse del sistema informatico in modo efficiente.
- **Capacità di Evolversi**: Un OS dovrebbe essere costruito in modo tale da permettere lo sviluppo, il test e l'introduzione di nuove funzioni di sistema in modo efficace allo stesso tempo senza interferire con il servizio.
- **Throughput**: Un OS dovrebbe essere costruito in modo da fornire il massimo throughput (Numero di compiti per unità di tempo).

### Funzionalità principali di un OS

- **Gestione delle Risorse**: Quando avviene l'accesso parallelo nel sistema operativo, significa che quando più utenti stanno accedendo al sistema, l'OS funge da Gestore delle Risorse. La sua responsabilità è fornire hardware all'utente. Riduce il carico nel sistema.
- **Gestione dei Processi**: Include vari compiti come la pianificazione e la terminazione dei processi. L'OS gestisce vari compiti contemporaneamente. Qui avviene la Pianificazione della CPU, cioè tutti i compiti verrebbero eseguiti da molti algoritmi utilizzati per la pianificazione.
- **Gestione dello Storage**: Il meccanismo del file system utilizzato per la gestione dello storage. NIFS, CFS, CIFS, NFS, ecc. sono alcuni file system. Tutti i dati sono memorizzati in vari tracce dei dischi rigidi che sono tutti gestiti dal gestore dello storage. Include un Disco Rigido.
- **Gestione della Memoria**: Si riferisce alla gestione della memoria primaria. Il sistema operativo deve tenere traccia di quanto memoria è stata utilizzata e da chi. Deve decidere quale processo ha bisogno di spazio in memoria e quanto. L'OS deve anche allocare e deallocare lo spazio in memoria.
- **Gestione della Sicurezza/Privacy**: La privacy è fornita anche dal sistema operativo utilizzando password in modo che applicazioni non autorizzate non possano accedere a programmi o dati. Ad esempio, Windows utilizza l'autenticazione **_Kerberos_** per prevenire accessi non autorizzati ai dati.

### Tipi di Sistemi Operativi

- **OS per Mainframe**:
Alla fascia alta ci sono i sistemi operativi per mainframe, quei computer di dimensioni da sala ancora trovati nei principali centri dati aziendali. Questi computer differiscono dai computer personali in termini di capacità di I/O. Un mainframe con 1000 dischi e milioni di gigabyte di dati non è insolito; un computer personale con queste specifiche sarebbe l'invidia dei suoi amici. I mainframe stanno anche facendo una sorta di ritorno come server di alta gamma, server per siti di commercio elettronico su larga scala e server per transazioni business-to-business. 

I sistemi operativi per mainframe sono fortemente orientati al trattamento di molti lavori contemporaneamente, la maggior parte dei quali richiede enormi quantità di I/O. Offrono tipicamente tre tipi di servizi: batch, elaborazione delle transazioni e time-sharing.

- **OS per Server**:
Un livello sotto ci sono i sistemi operativi per server. Essi girano su server, che sono computer personali molto grandi, workstation o anche mainframe. Servono più utenti contemporaneamente attraverso una rete e permettono agli utenti di condividere risorse hardware e software. I server possono fornire servizi di stampa, servizi di file o servizi Web. I provider di Internet eseguono molti server per supportare i loro clienti e i siti Web utilizzano server per memorizzare pagine Web e gestire le richieste in entrata. Tipici sistemi operativi per server sono Solaris, FreeBSD, Linux e Windows Server 201x.

- **OS per Multiprocessori**:
Un modo sempre più comune per ottenere una potenza di calcolo di alto livello è connettere più CPU in un unico sistema. A seconda di come sono collegati e cosa è condiviso, questi sistemi sono chiamati computer paralleli, multi-computer o multiprocessori. Hanno bisogno di sistemi operativi speciali, ma spesso questi sono variazioni dei sistemi operativi per server, con caratteristiche speciali per la comunicazione, la connettività e la coerenza.

- **OS per Computer Personali**:
La categoria successiva è il sistema operativo per computer personali. Quelli moderni supportano tutti la multiprogrammazione, spesso con decine di programmi avviati all'avvio. Il loro compito è fornire un buon supporto a un singolo utente. Sono ampiamente utilizzati per l'elaborazione di testi, fogli di calcolo, giochi e accesso a Internet. Esempi comuni sono Linux, FreeBSD, Windows 7, Windows 8 e OS X di Apple. I sistemi operativi per computer personali sono così ampiamente conosciuti che probabilmente non è necessaria molta introduzione. Molte persone non sono nemmeno consapevoli che esistono altri tipi.

- **OS per Sistemi Embedded**:
I sistemi embedded girano su computer che controllano dispositivi che non sono generalmente considerati computer e non accettano software installato dall'utente. Esempi tipici sono i forni a microonde, i televisori, le auto, i lettori DVD, i telefoni tradizionali e i lettori MP3. La principale proprietà che distingue i sistemi embedded dai dispositivi portatili è la certezza che nessun software non autorizzato verrà mai eseguito su di essi. Non puoi scaricare nuove applicazioni nel tuo forno a microonde—tutto il software è in ROM. Questo significa che non c'è bisogno di protezione tra le applicazioni, semplificando il design. Sistemi come Embedded Linux, QNX e VxWorks sono popolari in questo dominio.

- **OS per Smart Card**:
I sistemi operativi più piccoli girano su dispositivi smart card delle dimensioni di una carta di credito con chip CPU. Hanno vincoli molto severi di potenza di elaborazione e memoria. Alcuni sono alimentati tramite contatti nel lettore in cui sono inseriti, ma le smart card senza contatto sono alimentate per induzione, limitando notevolmente cosa possono fare. Alcuni possono gestire solo una singola funzione, come i pagamenti elettronici, ma altri possono gestire più funzioni. Spesso questi sono sistemi proprietari. Alcune smart card sono orientate a Java. Questo significa che la ROM sulla smart card contiene un interprete per la Java Virtual Machine (JVM). Gli applet Java (piccoli programmi) vengono scaricati sulla scheda e sono interpretati dall'interprete JVM. Alcune di queste schede possono gestire più applet Java contemporaneamente, portando alla multiprogrammazione e alla necessità di pianificarli. La gestione delle risorse e la protezione diventano anche un problema quando sono presenti due o più applet contemporaneamente. Questi problemi devono essere gestiti dal sistema operativo (di solito estremamente primitivo) presente sulla scheda.


## [Memoria e Archiviazione](Memory%20and%20Storage/readme.md)

### Memoria
Il termine _memoria_ si riferisce al componente all'interno del computer che consente l'accesso ai dati a breve termine. Potresti riconoscere questo componente come DRAM o memoria ad accesso casuale dinamico. Il computer esegue molte operazioni accedendo ai dati memorizzati nella sua memoria a breve termine. Alcuni esempi di tali operazioni includono la modifica di un documento, il caricamento delle applicazioni e la navigazione su Internet. La velocità e le prestazioni del tuo sistema dipendono dalla quantità di memoria installata sul computer.

Se hai una scrivania e un armadietto, la scrivania rappresenta la memoria del computer. Gli oggetti che devi utilizzare immediatamente sono tenuti sulla scrivania per un facile accesso. Tuttavia, non si può memorizzare molto su una scrivania a causa delle sue limitazioni di spazio.

### Archiviazione
Mentre la memoria si riferisce alla posizione dei dati a breve termine, _archiviazione_ è il componente all'interno del computer che ti consente di memorizzare e accedere ai dati a lungo termine. Di solito, l'archiviazione si presenta sotto forma di un'unità a stato solido o di un disco rigido. L'archiviazione contiene le tue applicazioni, il sistema operativo e i file in modo indefinito. I computer devono leggere e scrivere informazioni dal sistema di archiviazione, quindi la velocità dell'archiviazione determina quanto velocemente il sistema può avviarsi, caricare e accedere a ciò che hai salvato.

Mentre la scrivania rappresenta la memoria del computer, l'armadietto rappresenta l'archiviazione del computer. Esso conserva gli oggetti che devono essere salvati e memorizzati, ma non sono necessariamente necessari per un accesso immediato. La dimensione dell'armadietto significa che può contenere molte cose.

**Una distinzione importante** tra memoria e archiviazione è che la memoria si cancella quando il computer viene spento. D'altra parte, l'archiviazione rimane intatta indipendentemente da quante volte spegni il computer. Pertanto, nell'analogia tra scrivania e armadietto, qualsiasi file lasciato sulla tua scrivania verrà gettato via quando lasci l'ufficio. Tutto ciò che è nel tuo armadietto rimarrà.

### Memoria Virtuale
Al centro dei sistemi informatici c'è la memoria, lo spazio in cui i programmi vengono eseguiti e i dati sono memorizzati. Ma cosa succede quando i programmi che stai eseguendo e i dati con cui stai lavorando superano la capacità fisica della memoria del computer? Qui entra in gioco la memoria virtuale, che funge da estensione intelligente alla memoria del computer e ne migliora le capacità.

**Definizione e Scopo della Memoria Virtuale:**

La memoria virtuale è una tecnica di gestione della memoria impiegata dai sistemi operativi per superare le limitazioni della memoria fisica (RAM). Crea un'illusione per le applicazioni software di avere accesso a una quantità di memoria maggiore rispetto a quella fisicamente installata sul computer. In sostanza, consente ai programmi di utilizzare spazio di memoria al di là dei confini della RAM fisica del computer.

Lo scopo principale della memoria virtuale è consentire un multitasking efficiente e l'esecuzione di programmi più grandi, mantenendo al contempo la reattività del sistema. Questo viene ottenuto creando un'interazione fluida tra la RAM fisica e i dispositivi di archiviazione secondari, come il disco rigido o l'SSD.

**Come la Memoria Virtuale Estende la Memoria Fisica Disponibile:**

Considera la memoria virtuale come un ponte che collega la RAM del computer e l'archiviazione secondaria (unità disco). Quando esegui un programma, parti di esso vengono caricate nella memoria fisica più veloce (RAM). Tuttavia, non tutte le parti del programma potrebbero essere utilizzate immediatamente.

La memoria virtuale sfrutta questa situazione spostando le sezioni del programma che non sono attivamente utilizzate dalla RAM all'archiviazione secondaria, creando più spazio nella RAM per le parti che sono attivamente in uso. Questo processo è trasparente per l'utente e per i programmi in esecuzione. Quando le parti spostate sono nuovamente necessarie, vengono trasferite di nuovo nella RAM, mentre altre parti meno attive possono essere spostate nell'archiviazione secondaria.

Questo scambio dinamico di dati dentro e fuori dalla memoria fisica è gestito dal sistema operativo. Consente ai programmi di funzionare anche se sono più grandi della RAM disponibile, poiché il sistema operativo decide intelligentemente quali dati devono essere nella RAM per un'ottimale performance.

In sintesi, la memoria virtuale funge da strato di virtualizzazione che estende la memoria fisica disponibile trasferendo temporaneamente parti di programmi e dati tra la RAM e l'archiviazione secondaria. Questo processo garantisce che il computer possa gestire compiti più grandi e numerosi programmi simultaneamente, mantenendo al contempo prestazioni e reattività efficienti.

